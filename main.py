# -*- coding: utf-8 -*-

import json
import os
import queue
import re
import threading
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Tuple

import httpx
import gradio as gr
from dotenv import load_dotenv
from langchain_community.document_loaders import (
    UnstructuredPDFLoader,
    UnstructuredMarkdownLoader,
)
from langchain_openai import ChatOpenAI
from langchain_text_splitters import CharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from qdrant_manager import QdrantManager
from graph_builder import build_graph_html, extract_relations_from_chunks

try:
    import json_repair
except ImportError:
    raise ImportError(
        "The 'json_repair' library is required. Please install it with 'pip install json-repair'"
    )


load_dotenv(override=True)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
PROXY_URL = (
    os.getenv("PROXY_URL") or os.getenv("HTTPS_PROXY") or os.getenv("HTTP_PROXY")
)

# Set proxy environment variables for OpenAI client
if PROXY_URL:
    os.environ["HTTP_PROXY"] = PROXY_URL
    os.environ["HTTPS_PROXY"] = PROXY_URL
    # Exclude local services from proxy
    os.environ["NO_PROXY"] = "localhost,127.0.0.1,qdrant,neo4j"
    print(f"[DEBUG] Proxy configured: {PROXY_URL}")
    print(f"[DEBUG] NO_PROXY: {os.environ['NO_PROXY']}")

QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")

CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "800"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))
MAX_CHUNK_CHARS = int(os.getenv("MAX_CHUNK_CHARS", "2000"))
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "4"))

OPENAI_INGEST_MODEL = os.getenv("OPENAI_INGEST_MODEL", "gpt-4o-mini")
OPENAI_QA_MODEL = os.getenv("OPENAI_QA_MODEL", "gpt-4o-mini")


qdrant = QdrantManager(url=QDRANT_URL)

_UPLOAD_QUEUE: "queue.Queue[Dict[str, Any]]" = queue.Queue()
_UPLOAD_WORKER_STARTED = False


_LABEL_SAFE_RE = re.compile(r"[^0-9A-Za-z_\u0400-\u04FF]")


def build_http_client() -> httpx.Client:
    verify = os.getenv("SSL_CERT_FILE") or True
    if PROXY_URL:
        try:
            # Use 'all://' to catch all protocols
            # Use separate transports for proxy to ensure retries work
            proxy_transport = httpx.HTTPTransport(proxy=PROXY_URL, retries=3)
            return httpx.Client(
                transport=proxy_transport,
                timeout=60,
                verify=verify,
            )
        except (TypeError, ValueError) as e:
            print(f"[WARNING] Failed to configure proxy: {e}")
            transport = httpx.HTTPTransport(retries=3)
            return httpx.Client(timeout=60, verify=verify, transport=transport)
    transport = httpx.HTTPTransport(retries=3)
    return httpx.Client(timeout=60, verify=verify, transport=transport)


_HTTP_CLIENT = build_http_client()
_LLM_CACHE: Dict[Tuple[str, float], ChatOpenAI] = {}


def get_llm(model_name: str, temperature: float = 0.0) -> ChatOpenAI:
    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY is not set")
    key = (model_name, temperature)
    if key not in _LLM_CACHE:
        _LLM_CACHE[key] = ChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            api_key=OPENAI_API_KEY,
            http_client=_HTTP_CLIENT,
        )
    return _LLM_CACHE[key]


def normalize_whitespace(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()


def normalize_key(text: str) -> str:
    return normalize_whitespace(text).lower()


def sanitize_session_name(name: str) -> str:
    cleaned = normalize_whitespace(name)
    cleaned = cleaned.replace("`", "").replace('"', "'")
    return cleaned


def sanitize_label(value: str, fallback: str) -> str:
    cleaned = _LABEL_SAFE_RE.sub("_", value or "")
    cleaned = re.sub(r"_+", "_", cleaned).strip("_")
    return cleaned or fallback


def sanitize_properties(properties: Dict[str, Any]) -> Dict[str, Any]:
    cleaned: Dict[str, Any] = {}
    for key, value in (properties or {}).items():
        if isinstance(value, (str, int, float, bool)):
            cleaned[key] = value
        elif isinstance(value, list) and all(
            isinstance(item, (str, int, float, bool)) for item in value
        ):
            cleaned[key] = value
        else:
            cleaned[key] = str(value)
    return cleaned


def _ensure_upload_worker() -> None:
    global _UPLOAD_WORKER_STARTED
    if _UPLOAD_WORKER_STARTED:
        return
    _UPLOAD_WORKER_STARTED = True

    def _worker() -> None:
        while True:
            job = _UPLOAD_QUEUE.get()
            session_name = job["session_name"]
            files = job["files"]

            for file_path in files:
                document_name = os.path.splitext(os.path.basename(file_path))[0]
                try:
                    if not os.path.exists(file_path):
                        print(f"[WARNING] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                        qdrant.set_document_status(
                            session_name,
                            document_name,
                            "error",
                            error_message=f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}",
                        )
                        continue

                    # Update status to processing
                    qdrant.set_document_status(session_name, document_name, "processing")

                    result = process_document(file_path, session_name)

                    if result["status"] == "error":
                        print(f"[ERROR] {result['message']}")
                        qdrant.set_document_status(
                            session_name,
                            document_name,
                            "error",
                            error_message=result["message"],
                        )
                    elif result["status"] == "success":
                        print(f"[SUCCESS] {result['message']}")
                        # Status already updated in process_document
                except Exception as exc:
                    error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {exc}"
                    print(f"[ERROR] {error_msg}")
                    qdrant.set_document_status(
                        session_name, document_name, "error", error_message=str(exc)
                    )

            _UPLOAD_QUEUE.task_done()

    threading.Thread(target=_worker, name="upload-worker", daemon=True).start()


def _enqueue_upload(files: List[str], session_name: str) -> str:
    _ensure_upload_worker()
    job_id = str(uuid.uuid4())
    _UPLOAD_QUEUE.put({"job_id": job_id, "session_name": session_name, "files": files})
    return job_id


def strip_code_fences(text: str) -> str:
    cleaned = text.strip()
    if cleaned.startswith("```"):
        cleaned = re.sub(r"```(?:json)?\s*", "", cleaned)
        cleaned = re.sub(r"\s*```", "", cleaned)
    return cleaned.strip()


def normalize_medical_term(term: str) -> str:
    """Normalize medical terminology for better deduplication"""
    term = term.lower().strip()
    # Remove dosage units from names for better deduplication
    # e.g., "–∏–±—É–ø—Ä–æ—Ñ–µ–Ω 200–º–≥" -> "–∏–±—É–ø—Ä–æ—Ñ–µ–Ω"
    term = re.sub(r"\s*\d+\s*(–º–≥|–≥|–º–ª|—Ç–∞–±|–∫–∞–ø—Å|–º–∫–≥|–º–µ|–µ–¥)\b", "", term)
    # Remove extra whitespace
    term = re.sub(r"\s+", " ", term).strip()
    return term


def build_entity_id(session_name: str, entity_name: str, entity_type: str) -> str:
    # Normalize medical terms for better deduplication
    normalized_name = normalize_medical_term(entity_name)
    return f"{normalize_key(session_name)}::{normalize_key(entity_type)}::{normalize_key(normalized_name)}"


ENTITY_PROMPT = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –ª–µ—á–µ–Ω–∏—è).\n\n"
            "–¢–ò–ü–´ –°–£–©–ù–û–°–¢–ï–ô (–¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å–∏—Å—Ç–µ–º—ã):\n\n"
            "–ü–†–ï–ü–ê–†–ê–¢–´ –ò –í–ï–©–ï–°–¢–í–ê:\n"
            "- –ü—Ä–µ–ø–∞—Ä–∞—Ç: —Ç–æ—Ä–≥–æ–≤—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–µ–∫–∞—Ä—Å—Ç–≤ (–ù—É—Ä–æ—Ñ–µ–Ω, –ü–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª, –¶–∏–∫–ª–æ—Ñ–æ—Å—Ñ–∞–º–∏–¥)\n"
            "- –î–µ–π—Å—Ç–≤—É—é—â–µ–µ–í–µ—â–µ—Å—Ç–≤–æ: –∞–∫—Ç–∏–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–∏–±—É–ø—Ä–æ—Ñ–µ–Ω, –ø–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª)\n"
            "- –§–∞—Ä–º–∞–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è–ì—Ä—É–ø–ø–∞: –∫–ª–∞—Å—Å—ã –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤ (–ù–ü–í–°, –∞–Ω—Ç–∏–±–∏–æ—Ç–∏–∫–∏, —Ü–∏—Ç–æ—Å—Ç–∞—Ç–∏–∫–∏)\n"
            "- –õ–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è–§–æ—Ä–º–∞: —Ñ–æ—Ä–º–∞ –≤—ã–ø—É—Å–∫–∞ (—Ç–∞–±–ª–µ—Ç–∫–∏, —Å—É—Å–ø–µ–Ω–∑–∏—è, –∏–Ω—ä–µ–∫—Ü–∏—è)\n\n"
            "–ó–ê–ë–û–õ–ï–í–ê–ù–ò–Ø –ò –°–û–°–¢–û–Ø–ù–ò–Ø:\n"
            "- –ó–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ: –Ω–æ–∑–æ–ª–æ–≥–∏–∏ –∏ –¥–∏–∞–≥–Ω–æ–∑—ã (—Å–æ–ª–∏—Ç–∞—Ä–Ω–∞—è –ø–ª–∞–∑–º–æ—Ü–∏—Ç–æ–º–∞, –≥–∞—Å—Ç—Ä–∏—Ç, –¥–∏–∞–±–µ—Ç)\n"
            "- –°–∏–º–ø—Ç–æ–º: –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è (–±–æ–ª—å, –ª–∏—Ö–æ—Ä–∞–¥–∫–∞, –∫—Ä–æ–≤–æ—Ç–µ—á–µ–Ω–∏–µ, –∞–Ω–µ–º–∏—è)\n"
            "- –°–∏–Ω–¥—Ä–æ–º: —Å–∏–º–ø—Ç–æ–º–æ–∫–æ–º–ø–ª–µ–∫—Å—ã (–Ω–µ—Ñ—Ä–æ—Ç–∏—á–µ—Å–∫–∏–π —Å–∏–Ω–¥—Ä–æ–º, –∏–Ω—Ç–æ–∫—Å–∏–∫–∞—Ü–∏—è)\n"
            "- –°—Ç–∞–¥–∏—è–ó–∞–±–æ–ª–µ–≤–∞–Ω–∏—è: —Å—Ç–∞–¥–∏–∏ –∏ —Å—Ç–µ–ø–µ–Ω–∏ (I —Å—Ç–∞–¥–∏—è, —Ä–µ–º–∏—Å—Å–∏—è, –æ–±–æ—Å—Ç—Ä–µ–Ω–∏–µ)\n"
            "- –ö–æ–¥–ú–ö–ë: –∫–æ–¥—ã –ú–ö–ë-10 (C90.2, C90.3, E10)\n"
            "- –§–∏–∑–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ–°–æ—Å—Ç–æ—è–Ω–∏–µ: –æ—Å–æ–±—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è (–±–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å, –ª–∞–∫—Ç–∞—Ü–∏—è, –¥–µ—Ç—Å–∫–∏–π –≤–æ–∑—Ä–∞—Å—Ç)\n\n"
            "–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê:\n"
            "- –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π–ú–µ—Ç–æ–¥: –º–µ—Ç–æ–¥—ã –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (–ú–†–¢, –ö–¢, –±–∏–æ–ø—Å–∏—è, –£–ó–ò, —Ä–µ–Ω—Ç–≥–µ–Ω)\n"
            "- –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–π–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å: –∞–Ω–∞–ª–∏–∑—ã –∏ –º–∞—Ä–∫–µ—Ä—ã (–≥–µ–º–æ–≥–ª–æ–±–∏–Ω, –ú-–≥—Ä–∞–¥–∏–µ–Ω—Ç, –∫—Ä–µ–∞—Ç–∏–Ω–∏–Ω)\n"
            "- –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ–û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ: –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ (—Ç–æ–º–æ–≥—Ä–∞—Ñ, —ç–Ω–¥–æ—Å–∫–æ–ø)\n\n"
            "–õ–ï–ß–ï–ù–ò–ï:\n"
            "- –ú–µ—Ç–æ–¥–õ–µ—á–µ–Ω–∏—è: –≤–∏–¥—ã —Ç–µ—Ä–∞–ø–∏–∏ (–ª—É—á–µ–≤–∞—è —Ç–µ—Ä–∞–ø–∏—è, —Ö–∏–º–∏–æ—Ç–µ—Ä–∞–ø–∏—è, —Ö–∏—Ä—É—Ä–≥–∏—è)\n"
            "- –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è–ü—Ä–æ—Ü–µ–¥—É—Ä–∞: –ø—Ä–æ—Ü–µ–¥—É—Ä—ã (—Ç—Ä–µ–ø–∞–Ω–æ–±–∏–æ–ø—Å–∏—è, –ø—É–Ω–∫—Ü–∏—è, —Ä–µ–∑–µ–∫—Ü–∏—è)\n"
            "- –ü—Ä–æ—Ç–æ–∫–æ–ª–õ–µ—á–µ–Ω–∏—è: —Å—Ö–µ–º—ã –ª–µ—á–µ–Ω–∏—è (–ø—Ä–æ—Ç–æ–∫–æ–ª ASCT, —Ä–µ–∂–∏–º VRD)\n"
            "- –î–æ–∑–∏—Ä–æ–≤–∫–∞: –¥–æ–∑—ã –∏ —Å—Ö–µ–º—ã (200–º–≥ 3 —Ä–∞–∑–∞ –≤ –¥–µ–Ω—å, 40-50 –ì—Ä)\n"
            "- –°–ø–æ—Å–æ–±–ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è: –∫–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å (–≤–Ω—É—Ç—Ä—å, –≤/–≤, –º–µ—Å—Ç–Ω–æ, –ª—É—á–µ–≤–∞—è —Ç–µ—Ä–∞–ø–∏—è)\n"
            "- –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å–õ–µ—á–µ–Ω–∏—è: —Å—Ä–æ–∫–∏ (–∫—É—Ä—Å 21 –¥–µ–Ω—å, –Ω–µ –±–æ–ª–µ–µ 5 –¥–Ω–µ–π)\n\n"
            "–ü–û–ö–ê–ó–ê–ù–ò–Ø –ò –ü–†–û–¢–ò–í–û–ü–û–ö–ê–ó–ê–ù–ò–Ø:\n"
            "- –ü–æ–∫–∞–∑–∞–Ω–∏–µ: –∫–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è (–≥–æ–ª–æ–≤–Ω–∞—è –±–æ–ª—å, –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ–ø—É—Ö–æ–ª—å)\n"
            "- –ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏–µ: –∫–æ–≥–¥–∞ –ù–ï–õ–¨–ó–Ø (–±–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å, –ø–æ—á–µ—á–Ω–∞—è –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å)\n"
            "- –ü–æ–±–æ—á–Ω–æ–µ–¥–µ–π—Å—Ç–≤–∏–µ: –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã (—Ç–æ—à–Ω–æ—Ç–∞, –º–∏–µ–ª–æ—Å—É–ø—Ä–µ—Å—Å–∏—è)\n"
            "- –û—Å–ª–æ–∂–Ω–µ–Ω–∏–µ: —Ç—è–∂–µ–ª—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è (–∫—Ä–æ–≤–æ—Ç–µ—á–µ–Ω–∏–µ, –∏–Ω—Ñ–µ–∫—Ü–∏—è, —Ä–µ—Ü–∏–¥–∏–≤)\n\n"
            "–û–†–ì–ê–ù–ò–ó–ê–¶–ò–Ø –ü–û–ú–û–©–ò:\n"
            "- –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: —É—á—Ä–µ–∂–¥–µ–Ω–∏—è (–æ–Ω–∫–æ–¥–∏—Å–ø–∞–Ω—Å–µ—Ä, –ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞)\n"
            "- –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç: –≤—Ä–∞—á–∏ (–æ–Ω–∫–æ–ª–æ–≥, –≥–µ–º–∞—Ç–æ–ª–æ–≥, —Ö–∏—Ä—É—Ä–≥)\n"
            "- –í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è–∫–∞—Ç–µ–≥–æ—Ä–∏—è: –¥–ª—è –∫–æ–≥–æ (–≤–∑—Ä–æ—Å–ª—ã–µ, –¥–µ—Ç–∏ 6-12 –ª–µ—Ç)\n\n"
            "–ü–†–û–ß–ï–ï:\n"
            "- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å: –∫–æ–º–ø–∞–Ω–∏–∏-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–∏\n"
            "- –£—Ä–æ–≤–µ–Ω—å–î–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: —É—Ä–æ–≤–Ω–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (A, B, C, 1, 2)\n"
            "- –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (—Å –∞–ª–∫–æ–≥–æ–ª–µ–º, —Å –∞–Ω—Ç–∏–∫–æ–∞–≥—É–ª—è–Ω—Ç–∞–º–∏)\n\n"
            "–ü–†–ê–í–ò–õ–ê:\n"
            "1. –ò–∑–≤–ª–µ–∫–∞–π –í–°–ï —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π, –º–µ—Ç–æ–¥–æ–≤ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏, –ª–µ—á–µ–Ω–∏—è\n"
            "2. –î–ª—è –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ–≤ –∏–∑–≤–ª–µ–∫–∞–π –¥–æ–∑–∏—Ä–æ–≤–∫–∏, –ø–æ–∫–∞–∑–∞–Ω–∏—è, –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è\n"
            "3. –î–ª—è –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π –∏–∑–≤–ª–µ–∫–∞–π —Å–∏–º–ø—Ç–æ–º—ã, —Å—Ç–∞–¥–∏–∏, –∫–æ–¥—ã –ú–ö–ë\n"
            "4. –°–æ—Ö—Ä–∞–Ω—è–π —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤ properties\n"
            "5. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞\n\n"
            "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ - –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –º–∞—Å—Å–∏–≤:\n"
            "[\n"
            '  {{"name": "–°–æ–ª–∏—Ç–∞—Ä–Ω–∞—è –ø–ª–∞–∑–º–æ—Ü–∏—Ç–æ–º–∞", "type": "–ó–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ", "properties": {{"–∫–æ–¥_–º–∫–±": "C90.2"}}}},\n'
            '  {{"name": "–ª—É—á–µ–≤–∞—è —Ç–µ—Ä–∞–ø–∏—è", "type": "–ú–µ—Ç–æ–¥–õ–µ—á–µ–Ω–∏—è", "properties": {{"–¥–æ–∑–∞": "40-50 –ì—Ä"}}}},\n'
            '  {{"name": "–ú–†–¢", "type": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π–ú–µ—Ç–æ–¥", "properties": {{}}}},\n'
            '  {{"name": "–±–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å", "type": "–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏–µ", "properties": {{}}}}\n'
            "]\n\n"
            "–ï—Å–ª–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–µ—Ç, –≤–µ—Ä–Ω–∏: []",
        ),
        ("human", "–¢–µ–∫—Å—Ç:\n{text}"),
    ]
)


RELATION_PROMPT = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–µ–∂–¥—É –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏ (–∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –º–µ—Ç–æ–¥—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –ª–µ—á–µ–Ω–∏—è).\n\n"
            "–¢–ò–ü–´ –û–¢–ù–û–®–ï–ù–ò–ô (–∏—Å–ø–æ–ª—å–∑—É–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫, –≥–ª–∞–≥–æ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã):\n\n"
            "–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê:\n"
            "- –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä—É–µ—Ç—Å—è_–º–µ—Ç–æ–¥–æ–º / –≤—ã—è–≤–ª—è–µ—Ç—Å—è_—Å_–ø–æ–º–æ—â—å—é\n"
            "- —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è_—Å–∏–º–ø—Ç–æ–º–æ–º / –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è\n"
            "- –∏–º–µ–µ—Ç_—Å—Ç–∞–¥–∏—é / –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è_–∫–∞–∫\n"
            "- –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è_–ø–æ_–º–∫–± / —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç_–∫–æ–¥—É\n\n"
            "–õ–ï–ß–ï–ù–ò–ï –ò –ü–†–ò–ú–ï–ù–ï–ù–ò–ï:\n"
            "- –ª–µ—á–∏—Ç—Å—è_–º–µ—Ç–æ–¥–æ–º / –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è_—Ç–µ—Ä–∞–ø–∏—è\n"
            "- –ø–æ–∫–∞–∑–∞–Ω_–ø—Ä–∏ / –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è_–ø—Ä–∏ / –Ω–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è_–ø—Ä–∏\n"
            "- —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω_–ø—Ä–∏ / –ø–æ–º–æ–≥–∞–µ—Ç_–ø—Ä–∏ / —É—Å—Ç—Ä–∞–Ω—è–µ—Ç\n"
            "- –≤–∫–ª—é—á–∞–µ—Ç_–ø—Ä–æ—Ü–µ–¥—É—Ä—É / —Ç—Ä–µ–±—É–µ—Ç_–≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n\n"
            "–ü–†–û–¢–ò–í–û–ü–û–ö–ê–ó–ê–ù–ò–Ø –ò –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø:\n"
            "- –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω_–ø—Ä–∏ / –∑–∞–ø—Ä–µ—â–µ–Ω_–ø—Ä–∏\n"
            "- –Ω–µ_—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è_–ø—Ä–∏ / –æ–≥—Ä–∞–Ω–∏—á–µ–Ω_–ø—Ä–∏\n"
            "- —Ç—Ä–µ–±—É–µ—Ç_–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏_–ø—Ä–∏ / —Å_–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é_–ø—Ä–∏\n\n"
            "–ü–û–ë–û–ß–ù–´–ï –≠–§–§–ï–ö–¢–´ –ò –û–°–õ–û–ñ–ù–ï–ù–ò–Ø:\n"
            "- –≤—ã–∑—ã–≤–∞–µ—Ç_–ø–æ–±–æ—á–Ω—ã–π_—ç—Ñ—Ñ–µ–∫—Ç / –º–æ–∂–µ—Ç_–≤—ã–∑–≤–∞—Ç—å\n"
            "- –ø—Ä–∏–≤–æ–¥–∏—Ç_–∫_–æ—Å–ª–æ–∂–Ω–µ–Ω–∏—é / –≤—ã–∑—ã–≤–∞–µ—Ç_–æ—Å–ª–æ–∂–Ω–µ–Ω–∏–µ\n"
            "- –æ—Å–ª–æ–∂–Ω—è–µ—Ç—Å—è / –ø—Ä–æ–≥—Ä–µ—Å—Å–∏—Ä—É–µ—Ç_–≤\n\n"
            "–ü–†–ï–ü–ê–†–ê–¢–´:\n"
            "- —Å–æ–¥–µ—Ä–∂–∏—Ç_–≤–µ—â–µ—Å—Ç–≤–æ / –¥–µ–π—Å—Ç–≤—É—é—â–µ–µ_–≤–µ—â–µ—Å—Ç–≤–æ\n"
            "- –≤—Ö–æ–¥–∏—Ç_–≤_–≥—Ä—É–ø–ø—É / –æ—Ç–Ω–æ—Å–∏—Ç—Å—è_–∫_–∫–ª–∞—Å—Å—É\n"
            "- –≤—ã–ø—É—Å–∫–∞–µ—Ç—Å—è_–≤_—Ñ–æ—Ä–º–µ / –∏–º–µ–µ—Ç_—Ñ–æ—Ä–º—É\n"
            "- –Ω–∞–∑–Ω–∞—á–∞–µ—Ç—Å—è_–≤_–¥–æ–∑–µ / –∏–º–µ–µ—Ç_–¥–æ–∑–∏—Ä–æ–≤–∫—É\n"
            "- –ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è_—Å–ø–æ—Å–æ–±–æ–º / –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è\n"
            "- –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç_—Å / –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º_—Å\n\n"
            "–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ï –°–í–Ø–ó–ò:\n"
            "- —è–≤–ª—è–µ—Ç—Å—è_—Ñ–∞–∫—Ç–æ—Ä–æ–º_—Ä–∏—Å–∫–∞ / —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç_—Ä–∞–∑–≤–∏—Ç–∏—é\n"
            "- –∞—Å—Å–æ—Ü–∏–∏—Ä–æ–≤–∞–Ω_—Å / —Å–≤—è–∑–∞–Ω_—Å\n"
            "- –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ—Ç—Å—è_–æ—Ç / –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è_–æ—Ç\n"
            "- —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è_–≤ / –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç_–≤\n\n"
            "–û–†–ì–ê–ù–ò–ó–ê–¶–ò–Ø –ü–û–ú–û–©–ò:\n"
            "- –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è_—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–º / –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è_–≤—Ä–∞—á–æ–º\n"
            "- –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è_–≤_—É—á—Ä–µ–∂–¥–µ–Ω–∏–∏ / –¥–æ—Å—Ç—É–ø–µ–Ω_–≤\n"
            "- —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω_–¥–ª—è_–≤–æ–∑—Ä–∞—Å—Ç–∞ / –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω_–¥–ª—è\n"
            "- –∏–º–µ–µ—Ç_—É—Ä–æ–≤–µ–Ω—å_–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ / –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω_—É—Ä–æ–≤–Ω–µ–º\n\n"
            "–ü–†–ê–í–ò–õ–ê:\n"
            "1. –°–æ–∑–¥–∞–≤–∞–π –æ—Ç–Ω–æ—à–µ–Ω–∏—è –¢–û–õ–¨–ö–û –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ (–∏—Å–ø–æ–ª—å–∑—É–π id)\n"
            '2. –¢–∏–ø –æ—Ç–Ω–æ—à–µ–Ω–∏—è = –≥–ª–∞–≥–æ–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ (–ª–µ—á–∏—Ç—Å—è_–º–µ—Ç–æ–¥–æ–º, –∞ –Ω–µ "–ª–µ—á–µ–Ω–∏–µ")\n'
            "3. –î–æ–±–∞–≤–ª—è–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ properties (–¥–æ–∑—ã, —Å—Ä–æ–∫–∏, —É—Å–ª–æ–≤–∏—è)\n"
            "4. –ù–µ –¥—É–±–ª–∏—Ä—É–π –æ—Ç–Ω–æ—à–µ–Ω–∏—è\n\n"
            "–§–æ—Ä–º–∞—Ç - –≤–∞–ª–∏–¥–Ω—ã–π JSON:\n"
            "[\n"
            '  {{"source_id": "id1", "target_id": "id2", "type": "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä—É–µ—Ç—Å—è_–º–µ—Ç–æ–¥–æ–º",\n'
            '   "properties": {{"–∫–æ–Ω—Ç–µ–∫—Å—Ç": "–¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω–æ—Å—Ç–∏"}}}},\n'
            '  {{"source_id": "id3", "target_id": "id4", "type": "–ª–µ—á–∏—Ç—Å—è_–º–µ—Ç–æ–¥–æ–º",\n'
            '   "properties": {{"–¥–æ–∑–∞": "40-50 –ì—Ä"}}}}\n'
            "]\n\n"
            "–ï—Å–ª–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –Ω–µ—Ç: []",
        ),
        (
            "human",
            "–¢–µ–∫—Å—Ç:\n{text}\n\n"
            "–°—É—â–Ω–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —ç—Ç–∏ id –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö):\n{entities}",
        ),
    ]
)


QA_PROMPT = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "–¢—ã –æ—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. "
            "–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. "
            "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏, —á—Ç–æ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ —Ç–µ–∫—É—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. "
            "–û—Ç–≤–µ—á–∞–π –ø–æ-—Ä—É—Å—Å–∫–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏ –ø–æ –¥–µ–ª—É.",
        ),
        (
            "human",
            "–í–æ–ø—Ä–æ—Å:\n{question}\n\n" "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n{results}",
        ),
    ]
)


def extract_entities_parallel(
    chunks: List[str], llm: ChatOpenAI
) -> List[List[Dict[str, Any]]]:
    chain = ENTITY_PROMPT | llm | StrOutputParser()
    results: List[List[Dict[str, Any]]] = [[] for _ in chunks]

    def _extract(text: str) -> List[Dict[str, Any]]:
        response = chain.invoke({"text": text[:MAX_CHUNK_CHARS]})
        parsed = json_repair.loads(strip_code_fences(response))
        if not isinstance(parsed, list):
            return []
        clean_entities = []
        for entity in parsed:
            if not isinstance(entity, dict):
                continue
            name = str(entity.get("name", "")).strip()
            entity_type = str(entity.get("type", "")).strip()
            if not name or not entity_type:
                continue
            properties = entity.get("properties")
            if not isinstance(properties, dict):
                properties = {}
            properties = sanitize_properties(properties)
            clean_entities.append(
                {"name": name, "type": entity_type, "properties": properties}
            )
        return clean_entities

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(_extract, text): idx for idx, text in enumerate(chunks)
        }
        for future in as_completed(futures):
            idx = futures[future]
            try:
                results[idx] = future.result()
            except Exception as exc:
                print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π (—á–∞–Ω–∫ {idx + 1}): {exc}")
                results[idx] = []

    return results


def extract_relations_parallel(
    chunks: List[str],
    entities_by_chunk: List[List[Dict[str, Any]]],
    llm: ChatOpenAI,
) -> List[Dict[str, Any]]:
    chain = RELATION_PROMPT | llm | StrOutputParser()
    relations: List[Dict[str, Any]] = []

    def _extract(text: str, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        if len(entities) < 2:
            return []
        payload = [
            {"id": e["id"], "name": e["name"], "type": e["type"]} for e in entities
        ]
        response = chain.invoke(
            {
                "text": text[:MAX_CHUNK_CHARS],
                "entities": json.dumps(payload, ensure_ascii=False),
            }
        )
        parsed = json_repair.loads(strip_code_fences(response))
        if not isinstance(parsed, list):
            return []
        allowed_ids = {e["id"] for e in entities}
        clean_relations = []
        for rel in parsed:
            if not isinstance(rel, dict):
                continue
            source_id = str(rel.get("source_id", "")).strip()
            target_id = str(rel.get("target_id", "")).strip()
            rel_type = str(rel.get("type", "")).strip()
            if not source_id or not target_id or not rel_type:
                continue
            if source_id == target_id:
                continue
            if source_id not in allowed_ids or target_id not in allowed_ids:
                continue
            properties = rel.get("properties")
            if not isinstance(properties, dict):
                properties = {}
            properties = sanitize_properties(properties)
            clean_relations.append(
                {
                    "source_id": source_id,
                    "target_id": target_id,
                    "type": rel_type,
                    "properties": properties,
                }
            )
        return clean_relations

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {}
        for text, entities in zip(chunks, entities_by_chunk):
            futures[executor.submit(_extract, text, entities)] = True
        for future in as_completed(futures):
            try:
                relations.extend(future.result())
            except Exception as exc:
                print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π: {exc}")

    return relations


def process_document(file_path: str, session_name: str) -> Dict[str, Any]:
    """Process PDF or Markdown document and store in Qdrant"""
    document_name = os.path.splitext(os.path.basename(file_path))[0]

    # Detect file type and use appropriate loader
    file_extension = os.path.splitext(file_path)[1].lower()
    if file_extension == ".pdf":
        loader = UnstructuredPDFLoader(file_path)
    elif file_extension in [".md", ".markdown"]:
        loader = UnstructuredMarkdownLoader(file_path)
    else:
        return {
            "status": "error",
            "document": document_name,
            "message": f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_extension}. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ .pdf –∏–ª–∏ .md",
        }

    print(f"[DEBUG] Loading {file_extension} file: {file_path}")
    docs = loader.load()
    print(
        f"[DEBUG] Loaded {len(docs)} document(s) with total {sum(len(d.page_content) for d in docs)} characters"
    )

    splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = splitter.split_documents(docs)
    chunk_texts = [normalize_whitespace(c.page_content) for c in chunks]
    print(f"[DEBUG] Split into {len(chunks)} chunks")

    ingest_llm = get_llm(OPENAI_INGEST_MODEL, temperature=0.0)

    # Extract entities from chunks
    raw_entities_by_chunk = extract_entities_parallel(chunk_texts, ingest_llm)
    entities_by_chunk: List[List[Dict[str, Any]]] = []

    for chunk_entities in raw_entities_by_chunk:
        prepared_chunk: List[Dict[str, Any]] = []
        for entity in chunk_entities:
            entity_id = build_entity_id(session_name, entity["name"], entity["type"])
            prepared = {
                "id": entity_id,
                "name": entity["name"],
                "type": entity["type"],
                "properties": dict(entity.get("properties", {})),
            }
            prepared_chunk.append(prepared)
        entities_by_chunk.append(prepared_chunk)

    # Store in Qdrant
    qdrant.add_chunks(chunk_texts, session_name, document_name, entities_by_chunk)

    total_entities = sum(len(e) for e in entities_by_chunk)

    # Update status to completed
    qdrant.set_document_status(
        session_name,
        document_name,
        "completed",
        chunks=len(chunks),
        entities=total_entities,
    )

    print(
        f"[DEBUG] Stored {len(chunks)} chunks with {total_entities} entities in Qdrant"
    )

    return {
        "status": "success",
        "document": document_name,
        "chunks": len(chunks),
        "entities": total_entities,
        "message": f"–ì–æ—Ç–æ–≤–æ: {document_name}",
    }


def get_sessions() -> List[str]:
    return qdrant.get_sessions()


def get_documents(session_name: Optional[str]) -> List[List[Any]]:
    if not session_name:
        return []
    docs = qdrant.get_documents(session_name)
    data = []
    for doc in docs:
        # Format created_at to be more readable
        created = doc.get("created_at", "")
        if created:
            try:
                from datetime import datetime
                dt = datetime.fromisoformat(created.replace('Z', '+00:00'))
                created = dt.strftime("%Y-%m-%d %H:%M")
            except:
                created = created[:16] if len(created) > 16 else created

        status = doc.get("status", "unknown")
        # Add emoji for status
        status_display = {
            "queued": "üïí Queued",
            "processing": "‚è≥ Processing",
            "completed": "‚úÖ Completed",
            "error": "‚ùå Error",
            "unknown": "‚ùì Unknown",
        }.get(status, status)

        # Add error message to status if present
        if status == "error" and doc.get("error_message"):
            status_display += f" ({doc['error_message'][:50]}...)" if len(doc.get("error_message", "")) > 50 else f" ({doc.get('error_message', '')})"

        data.append([
            doc["name"],
            status_display,
            created,
            doc.get("chunks", 0),
            doc.get("entities", 0),
        ])
    return data


def create_session_ui(session_name: str):
    name = sanitize_session_name(session_name)
    if not name:
        return (
            "‚ùå –£–∫–∞–∂–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏.",
            gr.Dropdown(choices=get_sessions(), allow_custom_value=True),
            [],
        )
    sessions = get_sessions()
    if name not in sessions:
        sessions = [name] + sessions
    return (
        f"‚úÖ –°–µ—Å—Å–∏—è '{name}' –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.",
        gr.Dropdown(choices=sessions, value=name, allow_custom_value=True),
        get_documents(name),
    )


def on_session_change(session_name: str):
    if not session_name:
        return [], "‚ö†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ —Å–µ—Å—Å–∏—é."
    return get_documents(session_name), f"–¢–µ–∫—É—â–∞—è —Å–µ—Å—Å–∏—è: {session_name}"


def refresh_documents(session_name: str):
    if not session_name:
        return [], "‚ö†Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ —Å–µ—Å—Å–∏—é."
    return get_documents(session_name), f"–¢–µ–∫—É—â–∞—è —Å–µ—Å—Å–∏—è: {session_name}"


def upload_pdfs(files, session_name: str, progress=gr.Progress()):
    """Upload and process PDF or Markdown documents"""
    if not session_name:
        return "‚ùå –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ —Å–µ—Å—Å–∏—é.", get_documents(session_name)
    if not files:
        return "‚ùå –§–∞–π–ª—ã –Ω–µ –≤—ã–±—Ä–∞–Ω—ã.", get_documents(session_name)

    file_list = files if isinstance(files, list) else [files]
    file_paths: List[str] = []
    for file_obj in file_list:
        file_path = getattr(file_obj, "name", None) or str(file_obj)
        file_paths.append(file_path)

        # Set initial status as queued for each document
        document_name = os.path.splitext(os.path.basename(file_path))[0]
        qdrant.set_document_status(session_name, document_name, "queued")

    job_id = _enqueue_upload(file_paths, session_name)
    return (
        f"üïí –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å: {len(file_paths)} —Ñ–∞–π–ª(–æ–≤). "
        f"ID –∑–∞–¥–∞–Ω–∏—è: {job_id}. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –≤ —Ñ–æ–Ω–µ. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫—É '–û–±–Ω–æ–≤–∏—Ç—å' –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞.",
        get_documents(session_name),
    )


def delete_session(session_name: str, confirmed: bool):
    """Delete a session and all its data"""
    if not session_name:
        return (
            "‚ùå –í—ã–±–µ—Ä–∏—Ç–µ —Å–µ—Å—Å–∏—é –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.",
            gr.Dropdown(choices=get_sessions(), allow_custom_value=True),
            [],
            False,
        )
    if not confirmed:
        return (
            "‚ö†Ô∏è –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏.",
            gr.Dropdown(choices=get_sessions(), allow_custom_value=True),
            get_documents(session_name),
            False,
        )

    try:
        qdrant.delete_session(session_name)
        remaining_sessions = get_sessions()
        new_session = remaining_sessions[0] if remaining_sessions else None

        return (
            f"‚úÖ –°–µ—Å—Å–∏—è '{session_name}' —É–¥–∞–ª–µ–Ω–∞.",
            gr.Dropdown(
                choices=remaining_sessions,
                value=new_session,
                allow_custom_value=True,
            ),
            get_documents(new_session) if new_session else [],
            False,
        )
    except Exception as e:
        return (
            f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è: {e}",
            gr.Dropdown(choices=get_sessions(), allow_custom_value=True),
            [],
            False,
        )


def answer_question(question: str, session_name: str) -> Tuple[str, str]:
    """Answer question using Qdrant vector search and return answer + graph HTML"""

    print(f"[DEBUG] Question: {question}")

    # 1. Search Qdrant for similar chunks
    search_results = qdrant.search(question, session_name, k=5)

    print(f"[DEBUG] Found {len(search_results)} relevant chunks")

    if not search_results:
        return "–í —Ç–µ–∫—É—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞.", ""

    # 2. Collect all entities from retrieved chunks
    all_entities = {}
    for chunk_data in search_results:
        if "entities" in chunk_data and chunk_data["entities"]:
            for ent in chunk_data["entities"]:
                ent_key = f"{ent['name']}::{ent['type']}"
                if ent_key not in all_entities:
                    all_entities[ent_key] = ent

    entities = list(all_entities.values())

    # 3. Extract relations from chunks
    llm = get_llm(OPENAI_INGEST_MODEL, temperature=0.0)
    relations = extract_relations_from_chunks(search_results, llm, RELATION_PROMPT)

    print(f"[DEBUG] Extracted {len(entities)} entities, {len(relations)} relations")

    # 4. Build graph HTML
    graph_html = build_graph_html(entities, relations)

    # 5. Generate answer using LLM
    qa_llm = get_llm(OPENAI_QA_MODEL, temperature=0.2)
    chain = QA_PROMPT | qa_llm | StrOutputParser()

    # Format context from chunks
    context = "\n\n".join(
        [
            f"[–î–æ–∫—É–º–µ–Ω—Ç: {r['document_name']}, Chunk {r['chunk_id']}]\n{r['text']}"
            for r in search_results
        ]
    )

    result_text = f"{context}"
    answer = chain.invoke({"question": question, "results": result_text})

    print(f"[DEBUG] Generated answer")

    return answer, graph_html


def chat(
    message: str,
    history: List[Dict[str, str]],
    session_name: str,
    current_graph: str = "",
):
    if not session_name:
        return history, "", current_graph
    if not message.strip():
        return history, "", current_graph

    history = history or []
    history.append({"role": "user", "content": message})

    answer, graph_html = answer_question(message, session_name)
    history.append({"role": "assistant", "content": answer})

    return history, "", graph_html


with gr.Blocks() as demo:
    # Initialize with existing sessions
    initial_sessions = get_sessions()
    initial_session = initial_sessions[0] if initial_sessions else None
    initial_docs = get_documents(initial_session) if initial_session else []
    initial_status = f"–¢–µ–∫—É—â–∞—è —Å–µ—Å—Å–∏—è: {initial_session}" if initial_session else "–°–æ–∑–¥–∞–π—Ç–µ —Å–µ—Å—Å–∏—é."

    with gr.Sidebar():
        gr.Markdown("### –°–µ—Å—Å–∏–∏")
        session_name_input = gr.Textbox(
            label="–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–∏", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –§–∞—Ä–º_–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"
        )
        create_session_btn = gr.Button("–°–æ–∑–¥–∞—Ç—å —Å–µ—Å—Å–∏—é")
        session_selector = gr.Dropdown(
            choices=initial_sessions,
            value=initial_session,
            label="–¢–µ–∫—É—â–∞—è —Å–µ—Å—Å–∏—è",
            allow_custom_value=True,
        )
        session_status = gr.Textbox(
            label="–°—Ç–∞—Ç—É—Å —Å–µ—Å—Å–∏–∏",
            value=initial_status,
            interactive=False
        )

        gr.Markdown("### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        upload_files = gr.File(
            file_count="multiple",
            file_types=[".pdf", ".md", ".markdown"],
            label="PDF –∏ Markdown —Ñ–∞–π–ª—ã",
        )
        upload_btn = gr.Button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å")
        upload_status = gr.Textbox(label="–°—Ç–∞—Ç—É—Å –∑–∞–≥—Ä—É–∑–∫–∏", interactive=False)

        delete_confirm = gr.Checkbox(label="–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ", value=False)
        delete_session_btn = gr.Button("–£–¥–∞–ª–∏—Ç—å —Å–µ—Å—Å–∏—é", variant="stop")
        delete_status = gr.Textbox(label="–°—Ç–∞—Ç—É—Å —É–¥–∞–ª–µ–Ω–∏—è", interactive=False)

    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π")
            graph_output = gr.HTML(
                label="Knowledge Graph",
                value=(
                    "<div style='padding: 20px; text-align: center; color: #666;'>"
                    "–ì—Ä–∞—Ñ –±—É–¥–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤—ã –∑–∞–¥–∞–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å"
                    "</div>"
                ),
            )

        with gr.Column(scale=1):
            gr.Markdown("### –í–æ–ø—Ä–æ—Å—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º")
            chatbot = gr.Chatbot(type="messages", height=400)
            user_input = gr.Textbox(
                placeholder="–°–ø—Ä–æ—Å–∏—Ç–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...", label="–í–∞—à –≤–æ–ø—Ä–æ—Å"
            )
            ask_btn = gr.Button("–°–ø—Ä–æ—Å–∏—Ç—å")

            gr.Markdown("### –î–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å–µ—Å—Å–∏–∏")
            refresh_docs_btn = gr.Button("–û–±–Ω–æ–≤–∏—Ç—å")
            document_table = gr.Dataframe(
                headers=["–î–æ–∫—É–º–µ–Ω—Ç", "–°—Ç–∞—Ç—É—Å", "–°–æ–∑–¥–∞–Ω", "–ß–∞–Ω–∫–æ–≤", "–°—É—â–Ω–æ—Å—Ç–µ–π"],
                datatype=["str", "str", "str", "number", "number"],
                interactive=False,
                value=initial_docs,
            )

    create_session_btn.click(
        fn=create_session_ui,
        inputs=[session_name_input],
        outputs=[session_status, session_selector, document_table],
    )
    session_selector.change(
        fn=on_session_change,
        inputs=[session_selector],
        outputs=[document_table, session_status],
    )
    upload_btn.click(
        fn=upload_pdfs,
        inputs=[upload_files, session_selector],
        outputs=[upload_status, document_table],
    )
    refresh_docs_btn.click(
        fn=refresh_documents,
        inputs=[session_selector],
        outputs=[document_table, session_status],
    )
    delete_session_btn.click(
        fn=delete_session,
        inputs=[session_selector, delete_confirm],
        outputs=[delete_status, session_selector, document_table, delete_confirm],
    )

    ask_btn.click(
        fn=chat,
        inputs=[user_input, chatbot, session_selector, graph_output],
        outputs=[chatbot, user_input, graph_output],
    )
    user_input.submit(
        fn=chat,
        inputs=[user_input, chatbot, session_selector, graph_output],
        outputs=[chatbot, user_input, graph_output],
    )

    def init_ui():
        sessions = get_sessions()
        default_session = sessions[0] if sessions else None
        status = (
            f"–¢–µ–∫—É—â–∞—è —Å–µ—Å—Å–∏—è: {default_session}"
            if default_session
            else "–°–æ–∑–¥–∞–π—Ç–µ —Å–µ—Å—Å–∏—é."
        )
        docs = get_documents(default_session) if default_session else []
        return (
            gr.update(choices=sessions, value=default_session),
            docs,
            status,
        )

    demo.load(
        fn=init_ui,
        inputs=None,
        outputs=[session_selector, document_table, session_status],
    )


demo.launch(share=True)
