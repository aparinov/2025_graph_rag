# README: Эксперименты по инженерии онтологий с LLM

## 1. Обзор проекта

Этот проект представляет собой набор экспериментов для оценки и улучшения использования больших языковых моделей (LLM), в частности `gpt-4o-mini`, для задач инженерии онтологий. Скрипт, написанный на Python для Google Colab, реализует две ключевые идеи, основанные на выводах из академических статей (например, `LLMs4OL`).

**Основные цели:**
1.  Сравнить эффективность различных техник промптинга для задачи типизации терминов.
2.  Реализовать и протестировать автоматический пайплайн для генерации, валидации и исправления онтологий в формате OWL/TTL.

## 2. Описание экспериментов

Проект включает два основных эксперимента.

### Эксперимент 1: Сравнение Chain-of-Thought vs. Structured Prompting

Этот эксперимент фокусируется на **Задаче A** из статьи `LLMs4OL` — типизации сущностей. Мы сравниваем два подхода к составлению промптов для классификации терминов по заданным типам (например, "существительное" или "глагол").

*   **а) Chain-of-Thought (CoT) Prompting**:
    *   **Подход**: Модель просят "подумать пошагово" о значении и использовании термина, прежде чем выдать окончательный ответ. Это должно стимулировать более глубокий логический вывод.
    *   **Пример промпта**: `Classify the word '{term}'... Think step-by-step about its meaning... then output only the final class name.`

*   **б) Structured Prompting**:
    *   **Подход**: Модель получает жёсткие инструкции: классифицировать термин, выбрав один из строго определённого списка классов, и не выводить никакой лишней информации.
    *   **Пример промпта**: `Classify the word '{term}' into one of the following classes: {list}. Return exactly one class name... nothing else.`

**Метрики для оценки:**
*   **MAP@1 (Mean Average Precision @ 1)**: Доля случаев, когда первый предложенный моделью тип совпадает с правильным.
*   **% «несуществующих типов»**: Доля случаев, когда модель генерирует тип, отсутствующий в заранее заданном списке валидных классов.

### Эксперимент 2: Автоматический пайплайн генерации и валидации онтологий

Этот эксперимент реализует мини-пайплайн "генерация + проверка + исправление" для создания синтаксически корректных и логически состоятельных онтологий.

**Архитектура пайплайна:**
1.  **Генерация**: LLM получает список классов и отношений и генерирует на их основе онтологию в синтаксисе **Turtle (TTL)**. В промпте содержатся строгие требования к формату вывода (префиксы, определения классов и свойств).
2.  **Валидация**:
    *   Сгенерированный TTL-файл проверяется с помощью библиотеки `owlready2`.
    *   Для проверки на логическую состоятельность (consistency) используется внешний **ризонер Pellet**.
3.  **Автоматическое исправление**:
    *   Если на этапе валидации возникает синтаксическая ошибка или ризонер находит логическое противоречие, скрипт переходит в режим исправления.
    *   Невалидный TTL-код вместе с сообщением об ошибке отправляется обратно в LLM с просьбой исправить его.
4.  **Итерация**: Этот цикл повторяется несколько раз (`max_attempts`), пока не будет сгенерирована валидная онтология, или пока не будет достигнут лимит попыток.

## 3. Структура кода

*   `predict(term, style)`: Основная функция для **Эксперимента 1**. Принимает на вход термин и стиль промпта (`"cot"` или `"structured"`), возвращает предсказанный класс.
*   `generate_ttl(terms, relations)`: Часть **Эксперимента 2**. Формирует промпт и запрашивает у LLM генерацию онтологии в TTL-формате.
*   `check_ontology(file_path)`: Валидатор для **Эксперимента 2**. Использует `owlready2` и `sync_reasoner_pellet` для проверки файла. Возвращает кортеж с результатом проверки и деталями ошибок.
*   `auto_fix_ttl(terms, relations, max_attempts)`: Главная функция **Эксперимента 2**. Оркестрирует весь цикл генерации, проверки и исправления.

## 4. Установка и зависимости

Для запуска скрипта необходимы следующие компоненты:

1.  **Python-библиотеки**:
    ```bash
    pip install pandas openai owlready2
    ```

2.  **Java Development Kit (JDK)**:
    *   Ризонер Pellet, используемый библиотекой `owlready2`, требует установленной Java. Убедитесь, что JDK установлен и переменная `JAVA_HOME` прописана в системных переменных.

3.  **API ключ OpenAI**:
    *   Для работы с моделью `gpt-4o-mini` необходим API-ключ от OpenAI.

## 5. Как запустить

1.  **Установите зависимости**, перечисленные выше.
2.  **Настройте окружение**, убедившись, что Java доступна.
3.  **Вставьте ваш API-ключ OpenAI** в скрипт:
    ```python
    client = OpenAI(api_key="ВАШ_API_КЛЮЧ")
    ```
4.  **Запустите скрипт** в среде Google Colab или локально.
    *   Сначала будут загружены необходимые файлы `templates.json` и `label_mapper.json`.
    *   Затем будет выполнен Эксперимент 1, и результаты сравнения промптов будут выведены в виде таблицы.
    *   После этого запустится Эксперимент 2, и в консоли будут отображаться попытки генерации и валидации онтологии. В случае успеха будет выведен финальный TTL-код.

## 6. Результаты и выводы

*   **Эксперимент 1**: Предварительные результаты показывают, что **Structured Prompting** с явным указанием допустимых классов значительно превосходит Chain-of-Thought для задач с ограниченным набором ответов. В тестах этот подход достиг `MAP@1 = 1.0` и `0%` несуществующих типов, что демонстрирует важность ограничения "пространства вывода" для LLM.

*   **Эксперимент 2**: Реализованный пайплайн "генерация-проверка-исправление" оказался **жизнеспособным**. Он позволяет автоматически получать синтаксически и логически корректные онтологии, даже если первая попытка генерации LLM была неудачной. Этот подход может служить основой для более надежных систем автоматизированной инженерии онтологий.