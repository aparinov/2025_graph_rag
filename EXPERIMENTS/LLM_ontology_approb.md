# README: Прототип для оценки LLM в задачах инженерии онтологий

## 1. Обзор проекта

Этот проект представляет собой прототип для исследования и оценки эффективности больших языковых моделей (LLM) в двух ключевых задачах инженерии онтологий:
1.  **Типизация терминов (Term Typing)**: Классификация сущностей по заданным онтологическим типам.
2.  **Генерация онтологий (Ontology Generation)**: Создание формальных онтологий в синтаксисе OWL/TTL с последующей валидацией.

Прототип использует набор данных **GeoNames** из челленджа `LLMs4OL` (Task A) и модель `gpt-4o-mini` через OpenAI API.

## 2. Ключевые эксперименты

### Эксперимент 1: Сравнение техник промптинга для типизации терминов

Этот эксперимент сравнивает два подхода к составлению промптов для классификации географических названий по **топ-9 основным классам GeoNames** (A, H, L, P, R, S, T, U, V).

*   **а) Structured Prompting**:
    *   **Подход**: Модели предоставляется жёсткий, структурированный промпт с явным списком допустимых классов и требованием вернуть **только одну букву-класс** без лишних рассуждений.
    *   **Цель**: Минимизировать "творчество" модели и заставить её работать в режиме строгого классификатора.

*   **б) Chain-of-Thought (CoT) Prompting**:
    *   **Подход**: Модель просят сначала "подумать пошагово", а затем выдать ответ. Этот подход исследуется в двух вариантах:
        1.  **Zero-shot CoT**: Базовый CoT без примеров.
        2.  **Few-shot CoT с парсером**: Улучшенный вариант, где в промпт добавляются несколько примеров (few-shot), а из ответа модели извлекается первый подходящий символ с помощью регулярного выражения. Это делает вывод более стабильным.

**Метрики для оценки:**
*   **MAP@1**: Точность классификации (доля правильных ответов).
*   **% non-existent**: Доля ответов, где модель сгенерировала класс, отсутствующий в списке разрешённых.

### Эксперимент 2: Пайплайн автоматической генерации и исправления онтологий

Этот эксперимент реализует итеративный пайплайн "генерация → проверка → исправление" для создания синтаксически и логически корректных онтологий.

**Архитектура пайплайна:**
1.  **Генерация**: LLM получает на вход список классов и отношений и генерирует на их основе онтологию в формате **Turtle (TTL)**.
2.  **Валидация (Post-hoc проверка)**:
    *   Сгенерированный TTL-код загружается с помощью библиотеки `owlready2`.
    *   Запускается внешний логический **ризонер (Pellet)** для проверки на синтаксические ошибки и логические несоответствия (inconsistencies).
3.  **Автоматическое исправление (Auto-Fix)**:
    *   В случае, если валидация провалилась, невалидный TTL-код вместе с текстом ошибки отправляется обратно в LLM с инструкцией исправить его.
    *   Этот цикл повторяется несколько раз до тех пор, пока не будет получена валидная онтология.

## 3. Архитектура кода

*   **Загрузка и подготовка данных**:
    *   Скрипт автоматически скачивает и распаковывает датасет `LLMs4OL`.
    *   Реализован робастный механизм поиска и парсинга тестовых файлов для GeoNames (Task A), который может обрабатывать различные форматы (`.csv`, `.tsv`, `.jsonl`).
    *   В случае неудачи предусмотрен **fallback** на небольшой демонстрационный датасет.
    *   Данные нормализуются: детальные типы GeoNames (`P.PPLA`, `T.MT`, и т.д.) сводятся к 9 основным классам (`P`, `T`).

*   **Функции экспериментов**:
    *   `ask_llm()` и `eval_run()`: Реализуют базовое сравнение CoT и Structured Prompting.
    *   `ask_llm_cot()` и `eval_run_cot()`: Реализуют улучшенный CoT с few-shot примерами и парсером.
    *   `gen_ttl_from_llm()`, `try_reason()`, `auto_fix()`: Составляют ядро пайплайна для генерации и автоматического исправления онтологий.

## 4. Установка и запуск

1.  **Установите зависимости Python**:
    ```bash
    pip install openai pandas scikit-learn owlready2 rdflib
    ```

2.  **Установите Java Runtime Environment (JRE)**:
    *   `owlready2` требует Java для работы ризонера Pellet. В Google Colab это делается командой:
    ```bash
    apt update && apt install -y default-jre
    ```

3.  **Настройте API-ключ**:
    *   Вставьте ваш API-ключ от OpenAI в переменную `OPENAI_API_KEY`.

4.  **Запустите ячейки ноутбука** последовательно.

## 5. Предварительные результаты и выводы

*   **Structured Prompting показывает себя значительно лучше** для задач с ограниченным выбором. На топ-9 классах GeoNames он достиг **MAP@1 ~0.97**.
*   **Chain-of-Thought без ограничений проваливается**, генерируя много "несуществующих" типов. Однако его производительность значительно улучшается при использовании **few-shot примеров и парсера** для извлечения ответа.
*   Пайплайн **post-hoc проверки (LLM → TTL → ризонер)** с автоматическим исправлением подтвердил свою **жизнеспособность**, позволяя итеративно добиваться от LLM генерации валидной онтологии.

## 6. Следующие шаги

1.  **Масштабирование**: Провести полный прогон экспериментов на всём тестовом наборе GeoNames.
2.  **Детализация**: Перейти от 9 основных классов к более гранулярным (fine-grained) типам.
3.  **Сравнение моделей**: Провести аналогичные тесты на других LLM (например, Claude, Llama).
4.  **Расширение метрик**: Добавить формальную оценку качества онтологий с использованием **SHACL** для проверки ограничений и **OWL-метрик** (например, `expressivity`, `richness`).